
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>18 - Heterogeneous Treatment Effects and Personalization &#8212; Causal Inference for the Brave and True</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="19 - Evaluating Causal Models" href="19-Evaluating-Causal-Models.html" />
    <link rel="prev" title="17 - Predictive Models 101" href="17-Predictive-Models-101.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Causal Inference for the Brave and True</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="landing-page.html">
   Causal Inference for The Brave and True
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Part I - The Yang
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Introduction-To-Causality.html">
   01 - Introduction To Causality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-Randomised-Experiments.html">
   02 - Randomised Experiments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-Stats-Review-The-Most-Dangerous-Equation.html">
   03 - Stats Review: The Most Dangerous Equation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-Graphical-Causal-Models.html">
   04 - Graphical Causal Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-The-Unreasonable-Effectiveness-of-Linear-Regression.html">
   05 - The Unreasonable Effectiveness of Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Grouped-and-Dummy-Regression.html">
   06 - Grouped and Dummy Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-Beyond-Confounders.html">
   07 - Beyond Confounders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-Instrumental-Variables.html">
   08 - Instrumental Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-Non-Compliance-and-LATE.html">
   09 - Non Compliance and LATE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Matching.html">
   10 - Matching
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11-Propensity-Score.html">
   11 - Propensity Score
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12-Doubly-Robust-Estimation.html">
   12 - Doubly Robust Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13-Difference-in-Differences.html">
   13 - Difference-in-Differences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14-Panel-Data-and-Fixed-Effects.html">
   14 - Panel Data and Fixed Effects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15-Synthetic-Control.html">
   15 - Synthetic Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16-Regression-Discontinuity-Design.html">
   16 - Regression Discontinuity Design
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Part II - The Yin
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="17-Predictive-Models-101.html">
   17 - Predictive Models 101
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   18 - Heterogeneous Treatment Effects and Personalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19-Evaluating-Causal-Models.html">
   19 - Evaluating Causal Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20-Plug-and-Play-Estimators.html">
   20 - Plug-and-Play Estimators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21-Meta-Learners.html">
   21 - Meta Learners
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="22-Debiased-Orthogonal-Machine-Learning.html">
   22 - Debiased/Orthogonal Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="23-Challenges-with-Effect-Heterogeneity-and-Nonlinearity.html">
   23 - Challenges with Effect Heterogeneity and Nonlinearity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="24-The-Diff-in-Diff-Saga.html">
   24 - The Difference-in-Differences Saga
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Debiasing-with-Orthogonalization.html">
   Debiasing with Orthogonalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Debiasing-with-Propensity-Score.html">
   Debiasing with Propensity Score
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="When-Prediction-Fails.html">
   When Prediction Fails
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Prediction-Metrics-For-Causal-Models.html">
   Why Prediction Metrics are Dangerous For Causal Models
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Contribute
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://www.patreon.com/causal_inference_for_the_brave_and_true">
   Patreon
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/18-Heterogeneous-Treatment-Effects-and-Personalization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/matheusfacure/python-causality-handbook/issues/new?title=Issue%20on%20page%20%2F18-Heterogeneous-Treatment-Effects-and-Personalization.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/matheusfacure/python-causality-handbook/master?urlpath=tree/18-Heterogeneous-Treatment-Effects-and-Personalization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-predictions-to-causal-inference">
   From Predictions to Causal Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-ate-to-cate">
   From ATE to CATE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predicting-elasticity">
   Predicting Elasticity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-ideas">
   Key Ideas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contribute">
   Contribute
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="heterogeneous-treatment-effects-and-personalization">
<h1>18 - Heterogeneous Treatment Effects and Personalization<a class="headerlink" href="#heterogeneous-treatment-effects-and-personalization" title="Permalink to this headline">¶</a></h1>
<div class="section" id="from-predictions-to-causal-inference">
<h2>From Predictions to Causal Inference<a class="headerlink" href="#from-predictions-to-causal-inference" title="Permalink to this headline">¶</a></h2>
<p>In the last chapter, we briefly covered Machine Learning models. ML models are tools for what I called predictions or, more technically, estimating the conditional expectation function <span class="math notranslate nohighlight">\(E[Y|X]\)</span>. In other words, ML is incredibly useful when you want to map from a known input <span class="math notranslate nohighlight">\(X\)</span> (like an english sentence, sales this month, brain scan images) to an initially unknown but well defined output <span class="math notranslate nohighlight">\(Y\)</span> (like japanese sentences, sales next month or cancer diagnostics). So, if ML deals with predictions, or estimating <span class="math notranslate nohighlight">\(E[Y|X]\)</span>, for it to be useful, you have to frame whatever problem you want to solve with ML, as a prediction problem, a problem where estimating <span class="math notranslate nohighlight">\(E[Y|X]\)</span> is the key. We walked through such an example in the last chapter. There, we had to predict customer profitability from customer specific features: <span class="math notranslate nohighlight">\(E[NetValue|Age, Income, Region]\)</span>. This information was very useful, because it allowed us to focus our effort in engaging with profitable customers, while not doing business with non-profitable customers. Here, predicting profitability well is key.</p>
<p>Notice that it is a passive approach to estimation in the sense that you remove yourself from the data generating process. In our example, we assumed that customer profitability, <code class="docutils literal notranslate"><span class="pre">NetValue</span></code>, was given. All we had to do was estimate it. In other words, we assumed there was nothing we could do about the customer’s profitability other than predict it. We could not increase it, nor decrease it. But that is not always true. In fact, a lot of times, companies have levers they can use to increase customer profitability. These levers can range from a prime or cheaper customer service, to discount, prices or marketing. In the industry, it’s often the case that we are inserted in the data generating process. We can affect it. Hence, as data scientists working in the industry, we often have to answer what is the best course of action or what intervention to make in order to optimise some business metric, usually profitability or some other intermediary metric like conversion, costs or sales.</p>
<p>In this world, where we are not passive observes, estimating <span class="math notranslate nohighlight">\(E[Y|X]\)</span> is not the full picture. Here is where we enter causal inference. We need to add another piece to our conditional expectation function. That piece is precisely what models our participation in the data generating process. This piece is the treatment:</p>
<div class="math notranslate nohighlight">
\[
E[Y|X, T]
\]</div>
<p>We now have to make the distinction from context or exogenous features <span class="math notranslate nohighlight">\(X\)</span> and treatments <span class="math notranslate nohighlight">\(T\)</span>. Both impact the outcome <span class="math notranslate nohighlight">\(Y\)</span>, but while we have no control over <span class="math notranslate nohighlight">\(X\)</span>, we can decide what value <span class="math notranslate nohighlight">\(T\)</span> will take or at least intervene on it. To give a concrete example, <span class="math notranslate nohighlight">\(Y\)</span> could be sales in a day, <span class="math notranslate nohighlight">\(X\)</span> could be context features you can’t control, but that give you information about sales, like average sales in the previous days, and <span class="math notranslate nohighlight">\(T\)</span> is the treatment variable you can intervene on in order to increase sales, like price, item stock levels or marketing. Causal inference is then the process of estimating the causal relationship between <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> under context <span class="math notranslate nohighlight">\(X\)</span>. Once we’ve done that, optimizing <span class="math notranslate nohighlight">\(Y\)</span> is just a matter of setting the treatment <span class="math notranslate nohighlight">\(T\)</span> in an optimal way</p>
<div class="math notranslate nohighlight">
\[
\underset{T}{argmax} \ E[Y|X, T]
\]</div>
<p>In that sense, beyond the positive aspect of causal inference, we also have a normative motivation.</p>
<p>In Part I, we tried to answer questions like what is the value of schooling? Can law changes decrease smoking levels? Can we increase academic achievement by having a positive mindset? What is the impact of alcohol on mortality rates? All those questions are interesting from the purely scientific view of understanding how the world works. But there is also practical motivation behind them. If we know the impact of schooling on earnings, we can understand what is a reasonable price to pay for it. In math terms, what we are doing is estimating the causal inference of schooling and optimizing it: <span class="math notranslate nohighlight">\(\underset{Educ}{argmax} \ E[Income|X, Educ]\)</span>.</p>
<p>Part I was focused in answering if a treatment was overall positive, strong or zero. For example, we wanted to know if investing in education was, in general, a good idea or not. Also in Part I, the role of <span class="math notranslate nohighlight">\(X\)</span> was twofold. First, <span class="math notranslate nohighlight">\(X\)</span> could contain confounders, in which case, the causal effect was only identifiable if we accounted for or adjusted for <span class="math notranslate nohighlight">\(X\)</span>. Or, <span class="math notranslate nohighlight">\(X\)</span> could act to reduce the variance of the causal estimation. If <span class="math notranslate nohighlight">\(X\)</span> is a good predictor of <span class="math notranslate nohighlight">\(Y\)</span>, we can use it to explain away variance in <span class="math notranslate nohighlight">\(Y\)</span>, making the causal effect more apparent.</p>
<p>Now, things will become less black and white. We want more than just the average treatment effect. We will allow the treatment to impact positively some people but not others. Context features <span class="math notranslate nohighlight">\(X\)</span> will play a role in defining different profiles of units and each profile might respond differently to the treatment. We now want to personalize the treatment, giving it only to those units that best respond to it. We are going from a world where all we cared about was the average treatment effect to one where we want the heterogeneous treatment effect.</p>
</div>
<div class="section" id="from-ate-to-cate">
<h2>From ATE to CATE<a class="headerlink" href="#from-ate-to-cate" title="Permalink to this headline">¶</a></h2>
<p>So far, we’ve every time we’ve estimated the causal impact of a treatment, it was the average treatment effect (or, sometime, the local average treatment effect):</p>
<div class="math notranslate nohighlight">
\[
E[Y_1−Y_0]
\]</div>
<p>or the continuous treatment equivalent</p>
<div class="math notranslate nohighlight">
\[
E[y'(t)]
\]</div>
<p>where <span class="math notranslate nohighlight">\(y'(t)\)</span> is the treatment derivative of the response function or outcome. We’ve learned techniques to uncover the general effectiveness of a treatment. ATE estimation is the bedrock of causal inference. It’s a super useful tool for the decision making problem we refer to as program evaluation. We want to know if we should roll out a treatment to the entire population or not. Don’t be confused by the public policy terms. The same technique to estimate the effectiveness of a national education or health program can also be used to know the effect of launching a new product on a company’s bottom line. The key thing to note here is that the decision we want to inform is if we should treat or not.</p>
<p>Now, we will try to inform another type of decision: <strong>who</strong> do we treat? Now, we allow the decision to change from one unit to another. It might be beneficial to treat one unit but not another. We want to personalize the treatment. In more technical terms, we want to estimate the Conditional Average Treatment Effect (CATE)</p>
<div class="math notranslate nohighlight">
\[
E[Y_1−Y_0 | X] \ \text{or} \ E[y'(t)|X]
\]</div>
<p>The conditioning on <span class="math notranslate nohighlight">\(X\)</span> means that we now allow the treatment effect to be different depending on the characteristics of each unit. Again, here, we believe that not all entities respond equally well to the treatment. We want to leverage that heterogeneity. We want to treat only the right units (in the binary case) or figure out what is the optimal treatment dosage for each unit (in the continuous case).</p>
<p>For instance, if you are a bank that has to decide the loan each customer is eligible for, you can be damn sure that it’s not a good idea to give loads of money to everyone - although it might be reasonable for some. You will have to be smart with your treatment (loan amount). Perhaps, depending on the customer credit score (<span class="math notranslate nohighlight">\(X\)</span>), you can figure out what is the proper loan dosage. Of course, you don’t need to be a big institution to leverage personalisation. There’s no shortage of examples where it applies. What days of the year should you do sales? How much should you charge for whatever product? How much exercise is too much exercise for each person?</p>
<p>Think of it this way. You have a bunch of customers and a treatment (price, discount, loan,…). You want to personalize the treatment, for example, give different discounts to different customers.</p>
<p><img alt="img" src="_images/customers.png" /></p>
<p>To do that, you have to segment your customers. You have created groups that respond differently to your treatment. For example, you want to find customers that respond well to discounts and customer’s who respond poorly to it. Well, the customer’s response to a treatment is given by the conditional treatment effect <span class="math notranslate nohighlight">\(\frac{\delta Y}{ \delta T}\)</span>. So, we could somehow estimate that for each customer, we could group together those that respond great to the treatment (high treatment effect) and those that don’t respond very well to it. If we did that, we would split the customers space somewhat like the following image.</p>
<p><img alt="img" src="_images/elast-partition.png" /></p>
<p>That would be wonderful because now we would be able to estimate different treatment effects or elasticities on each partition. And notice that the elasticity is just the slope of the line or function that goes from <span class="math notranslate nohighlight">\(T\)</span> to <span class="math notranslate nohighlight">\(Y\)</span>. So, if we can produce partitions where the slope or elasticity differs, it means that entities on those partitions have different responsiveness to the treatment.</p>
<p><img alt="img" src="_images/elast-split.png" /></p>
<p>In other words, what you want is to move away from predicting <span class="math notranslate nohighlight">\(Y\)</span> in its raw form and start to predict the derivative of <span class="math notranslate nohighlight">\(Y\)</span> on <span class="math notranslate nohighlight">\(T\)</span>,  <span class="math notranslate nohighlight">\(\frac{\delta Y}{ \delta T}\)</span> for each unit. For example, suppose that <span class="math notranslate nohighlight">\(Y\)</span> is ice cream sales, <span class="math notranslate nohighlight">\(T\)</span> is ice cream price and each unit <span class="math notranslate nohighlight">\(i\)</span> is a day. Let’s set moral issues aside, for the sake of the argument, and pretend that you can change the price of ice cream every day. If you can somehow find the days where <span class="math notranslate nohighlight">\(\frac{\delta Sales}{ \delta Price}\)</span> <strong>is low</strong>, you can <strong>increase prices</strong> without losing much sales on those days. Perhaps you do this already, say, when you increase them during the holiday season. The point being, it’s useful to differentiate days in terms of the price elasticity because it gives you some basis on how to set prices in an optimal way.</p>
<p>Ok, you might say, but this is kind of tricky. How can I predict elasticity <span class="math notranslate nohighlight">\(\frac{\delta Sales}{ \delta Price}\)</span> if I can’t see it? That’s a very good point. Elasticity is essentially non observable on a unit level. Not only that, it’s a strange concept. We are much more accustomed to thinking in terms of raw quantities rather than in terms of change rates of those same quantities. So, to conceptualize elasticity better, here is a little trick. You can think about each entity as having a <span class="math notranslate nohighlight">\(Y_i\)</span> value, sales in our example, but also an individual elasticity <span class="math notranslate nohighlight">\(\frac{delta Y_i}{\delta T_i}\)</span>. The elasticity is how much <span class="math notranslate nohighlight">\(Y\)</span> changes with <span class="math notranslate nohighlight">\(T\)</span>, so you can think about each entity also having a slope coefficient associated to it <span class="math notranslate nohighlight">\(\frac{\delta Y}{ \delta T}_i\)</span>. In our example, we would say each day has a slope coefficient of price on sales.</p>
<p><img alt="img" src="_images/elasticity.png" /></p>
<p>Of course, we can’t see those individual slope coefficients. For us to see the individual slopes, we would have to observe each day under two different prices and calculate how the sales changes for each of those prices.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta Y_i}{ \delta T_i} \approx \frac{Y(T_i) - Y(T_i + \epsilon)}{T_i - (T_i + \epsilon)}
\]</div>
<p>This is the fundamental problem of causal inference all over again. We can’t ever see the same unit under different treatment conditions. So, what can we do?</p>
</div>
<div class="section" id="predicting-elasticity">
<h2>Predicting Elasticity<a class="headerlink" href="#predicting-elasticity" title="Permalink to this headline">¶</a></h2>
<p>We got ourselves into a complicated situation here. We’ve agreed that we need to predict <span class="math notranslate nohighlight">\(\frac{\delta Y_i}{ \delta T_i}\)</span>, which is sadly not observable. So it’s not like we could use a ML algorithm and plug that as it’s target. But maybe we don’t need to observe <span class="math notranslate nohighlight">\(\frac{\delta Y_i}{ \delta T_i}\)</span> in order to predict it</p>
<p>Here is an idea. What if we use linear regression?</p>
<p><img alt="img" src="_images/linear-fix.png" /></p>
<p>Let’s say you fit the following linear model to your data.</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 t_i + \beta_2 X_i + e_i
\]</div>
<p>If you differentiate it on the treatment, you will end up with</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y_i}{\delta t_i} = \beta_1 
\]</div>
<p>And since you can estimate the model above to get <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, we might even be as bold as to say that <strong>you can predict elasticity even though you can’t observe it</strong>. In the case above, it is a rather simple prediction, that is, we are predicting the constant value <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> for everyone. That’s something, but not yet what we want. That’s the ATE, not the CATE. This doesn’t help us in our task of grouping entities according to how responsive they are to the treatment, simply because everyone gets the same elasticity prediction. However, we can do the following simple change</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 t_i + \beta_2 X_i + \beta_3 t_i X_i  + e_i
\]</div>
<p>Which would in turn give us the following elasticity prediction</p>
<div class="math notranslate nohighlight">
\[
\widehat{\frac{\delta y_i}{\delta t_i}} = \hat{\beta_1} + \hat{\beta_3}X_i
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\beta_3\)</span> is a vector coefficient for the features in <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Now each entity defined by a different <span class="math notranslate nohighlight">\(X_i\)</span> will have a different elasticity prediction. In other words, the elasticity prediction will change as <span class="math notranslate nohighlight">\(X\)</span> changes. Alas, regression can gives us a way of estimating the CATE <span class="math notranslate nohighlight">\(E[y'(t)|X]\)</span>.</p>
<p>We are finally getting somewhere. The model above allows us to make an elasticity prediction for each of our entities. With those predictions we can make more useful groups. We can take the units with high predicted elasticity and group them together. We can do the same with the ones that have low predicted elasticity. Finally, with our elasticity predictions, we can group entities by how much we think they will respond to the treatment.</p>
<p>Enough of theory for now. It’s time to walk through an example of how to make this sort of elasticity model. Let’s consider our ice cream example. Each unit <span class="math notranslate nohighlight">\(i\)</span> is a day. For each day, we know if it’s a weekday or not, what was the cost we had to make the ice cream (you can think of cost as a proxy for quality) and the average temperature for that day. Those will be our feature space <span class="math notranslate nohighlight">\(X\)</span>. Then, we have our treatment, price, and our outcome, the number of ice cream sold. For this example, we will consider that the treatment is randomized, just so that we don’t have to worry about bias for now.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prices_rnd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./data/ice_cream_sales_rnd.csv&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prices_rnd</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">prices_rnd</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 5)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25.8</td>
      <td>1</td>
      <td>0.3</td>
      <td>7</td>
      <td>230</td>
    </tr>
    <tr>
      <th>1</th>
      <td>22.7</td>
      <td>3</td>
      <td>0.5</td>
      <td>4</td>
      <td>190</td>
    </tr>
    <tr>
      <th>2</th>
      <td>33.7</td>
      <td>7</td>
      <td>1.0</td>
      <td>5</td>
      <td>237</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.0</td>
      <td>4</td>
      <td>0.5</td>
      <td>5</td>
      <td>193</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.4</td>
      <td>1</td>
      <td>1.0</td>
      <td>3</td>
      <td>252</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Remember our goal here: we need to decide when to charge more and when to charge less depending on the day specific <code class="docutils literal notranslate"><span class="pre">features</span></code>, <code class="docutils literal notranslate"><span class="pre">temp</span></code> <code class="docutils literal notranslate"><span class="pre">weekday</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>. If that’s the goal, the treatment effect heterogeneity model needs to be evaluated with respect to its usefulness in achieving this goal. We will get to that in a moment (and a lot more in the next chapter). For now, let’s just split the dataset into a training and testing set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">prices_rnd</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have our training data, we need to make a model which will allow us to distinguish days with high price elasticity from days with low price elasticity. Our approach to do that will be to simply predict price elasticity. How exactly? First, let’s consider using the following linear model</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \pmb{\beta_2}X_i + e_i
\]</div>
<p>If we inspect the parameters of this model, we can see what our predicted elasticity will look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price + temp+C(weekday)+cost&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">m1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>  186.7113</td> <td>    1.770</td> <td>  105.499</td> <td> 0.000</td> <td>  183.241</td> <td>  190.181</td>
</tr>
<tr>
  <th>C(weekday)[T.2]</th> <td>  -25.0512</td> <td>    0.924</td> <td>  -27.114</td> <td> 0.000</td> <td>  -26.863</td> <td>  -23.240</td>
</tr>
<tr>
  <th>C(weekday)[T.3]</th> <td>  -24.5834</td> <td>    0.901</td> <td>  -27.282</td> <td> 0.000</td> <td>  -26.350</td> <td>  -22.817</td>
</tr>
<tr>
  <th>C(weekday)[T.4]</th> <td>  -24.3807</td> <td>    0.897</td> <td>  -27.195</td> <td> 0.000</td> <td>  -26.138</td> <td>  -22.623</td>
</tr>
<tr>
  <th>C(weekday)[T.5]</th> <td>  -24.9036</td> <td>    0.894</td> <td>  -27.850</td> <td> 0.000</td> <td>  -26.657</td> <td>  -23.150</td>
</tr>
<tr>
  <th>C(weekday)[T.6]</th> <td>  -24.0921</td> <td>    0.903</td> <td>  -26.693</td> <td> 0.000</td> <td>  -25.862</td> <td>  -22.323</td>
</tr>
<tr>
  <th>C(weekday)[T.7]</th> <td>   -0.8635</td> <td>    0.888</td> <td>   -0.972</td> <td> 0.331</td> <td>   -2.605</td> <td>    0.878</td>
</tr>
<tr>
  <th>price</th>           <td>   -2.7515</td> <td>    0.106</td> <td>  -25.970</td> <td> 0.000</td> <td>   -2.959</td> <td>   -2.544</td>
</tr>
<tr>
  <th>temp</th>            <td>    1.9848</td> <td>    0.060</td> <td>   33.117</td> <td> 0.000</td> <td>    1.867</td> <td>    2.102</td>
</tr>
<tr>
  <th>cost</th>            <td>    4.4718</td> <td>    0.528</td> <td>    8.462</td> <td> 0.000</td> <td>    3.436</td> <td>    5.508</td>
</tr>
</table></div></div>
</div>
<p>For <span class="math notranslate nohighlight">\(m1\)</span>, the predicted price elasticity <span class="math notranslate nohighlight">\(\widehat{\dfrac{\delta y_i}{\delta t_i}}\)</span> will be given by <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, which is -2.75, in our case. This means that for each additional BRL we charge for our ice cream, we should expect sales to go down by about 3 units.</p>
<p>Notice how this <span class="math notranslate nohighlight">\(m1\)</span> predicts the exact same elasticity for everyone. Hence, it is not a very good model if we want to know on which days people are less sensitive to ice cream prices. It estimates the ATE when what we need here is the CATE. Remember that our goal is to partition the entities in such a way that we can personalise and optimise our treatment (price) for each individual partition. If every prediction is the same, there is no partitioning we can make. We are not distinguishing sensitive from non sensitive units. To correct for that, consider our second model:</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \beta_2 price_i * temp_i * + \pmb{\beta_3}X_i + e_i
\]</div>
<p>This second model includes an <strong>interaction term</strong> between price and temperature. This means that it allows the elasticity to differ for different temperatures. What we are effectively saying here is that people are more or less sensitive to price increases depending on the temperature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m2</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price*temp + C(weekday) + cost&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">m2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>  192.4767</td> <td>    4.371</td> <td>   44.037</td> <td> 0.000</td> <td>  183.907</td> <td>  201.046</td>
</tr>
<tr>
  <th>C(weekday)[T.2]</th> <td>  -25.0805</td> <td>    0.924</td> <td>  -27.143</td> <td> 0.000</td> <td>  -26.892</td> <td>  -23.269</td>
</tr>
<tr>
  <th>C(weekday)[T.3]</th> <td>  -24.5871</td> <td>    0.901</td> <td>  -27.290</td> <td> 0.000</td> <td>  -26.354</td> <td>  -22.821</td>
</tr>
<tr>
  <th>C(weekday)[T.4]</th> <td>  -24.4225</td> <td>    0.897</td> <td>  -27.231</td> <td> 0.000</td> <td>  -26.181</td> <td>  -22.664</td>
</tr>
<tr>
  <th>C(weekday)[T.5]</th> <td>  -24.8953</td> <td>    0.894</td> <td>  -27.844</td> <td> 0.000</td> <td>  -26.648</td> <td>  -23.142</td>
</tr>
<tr>
  <th>C(weekday)[T.6]</th> <td>  -24.1269</td> <td>    0.903</td> <td>  -26.726</td> <td> 0.000</td> <td>  -25.897</td> <td>  -22.357</td>
</tr>
<tr>
  <th>C(weekday)[T.7]</th> <td>   -0.8581</td> <td>    0.888</td> <td>   -0.966</td> <td> 0.334</td> <td>   -2.599</td> <td>    0.883</td>
</tr>
<tr>
  <th>price</th>           <td>   -3.6299</td> <td>    0.618</td> <td>   -5.873</td> <td> 0.000</td> <td>   -4.842</td> <td>   -2.418</td>
</tr>
<tr>
  <th>temp</th>            <td>    1.7459</td> <td>    0.176</td> <td>    9.912</td> <td> 0.000</td> <td>    1.401</td> <td>    2.091</td>
</tr>
<tr>
  <th>price:temp</th>      <td>    0.0366</td> <td>    0.025</td> <td>    1.443</td> <td> 0.149</td> <td>   -0.013</td> <td>    0.086</td>
</tr>
<tr>
  <th>cost</th>            <td>    4.4558</td> <td>    0.529</td> <td>    8.431</td> <td> 0.000</td> <td>    3.420</td> <td>    5.492</td>
</tr>
</table></div></div>
</div>
<p>Once we estimate the model, the predicted elasticity is given by</p>
<div class="math notranslate nohighlight">
\[
\widehat{\frac{\delta sales_i}{\delta price_i}} = \hat{\beta_1} + \hat{\beta_3}temp_i
\]</div>
<p>Notice that <span class="math notranslate nohighlight">\(\hat{\beta_3}\)</span> is positive 0,03 and the baseline elasticity <span class="math notranslate nohighlight">\(\beta_1\)</span> (the elasticity at <span class="math notranslate nohighlight">\(0C^o\)</span>) is -3.6. This means that, on average, as we increase price, sales go down, which makes sense. It also means that for each additional degree in temperature, people become less sensitive to price increases on ice cream (although not by much). For example, at <span class="math notranslate nohighlight">\(25C^o\)</span>, for each additional BRL we charge, our sales go down by 2.8 units <span class="math notranslate nohighlight">\((-3.6 + (0.03 * 25))\)</span>. But at  <span class="math notranslate nohighlight">\(35C^o\)</span>, for each additional BRL we charge, they go down only by 2.5 units <span class="math notranslate nohighlight">\((-3.6 + (0.03 * 35))\)</span>. This is also sort of intuitive. As the days get hotter and hotter, people are willing to pay more for ice cream.</p>
<p>We can go even further. The next model includes interaction terms on all the feature space. This means that elasticity will change with temperature, day of the week and cost.</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \pmb{\beta_2 X_i}*price_i + \pmb{\beta_3}X_i + e_i
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m3</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price*cost + price*C(weekday) + price*temp&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>According to the above model, the unit level elasticity, or CATE, would be given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta Sales}{\delta Price} = \beta_1 + \pmb{\beta_2 X_i}
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\beta_1\)</span> is the price coefficient and <span class="math notranslate nohighlight">\(\pmb{\beta_2}\)</span> is the vector for the interaction coefficients.</p>
<p>Finally, let’s see how to actually make those elasticity predictions. One way is to extract the elasticity parameters from the model and use the formula above. However, we will resort to a more general approximation. Since elasticity is nothing more than the derivative of the outcome on treatment, we can use the definition of the derivative.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y}{\delta t} = \dfrac{y(t+\epsilon) - y(t)}{ (t + \epsilon) - t }
\]</div>
<p>with <span class="math notranslate nohighlight">\(\epsilon\)</span> going to zero. We can approximate this definition by replacing <span class="math notranslate nohighlight">\(\epsilon\)</span> by 1.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y}{\delta t} \approx \hat{y}(t+1) - \hat{y}(t)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}\)</span> is given by our model’s predictions. In words, I’ll make two predictions with my models: one passing the original data and another passing the original data but with the treatment incremented by one unit. The difference between those predictions is my CATE prediction.</p>
<p>Below, you can see a function for doing that. Since we’ve used the train set to estimate our model, we will now make predictions on the test set. First, let’s use our first, ATE model, <span class="math notranslate nohighlight">\(m1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pred_elasticity</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
        <span class="s2">&quot;pred_elast&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">t</span><span class="p">:</span><span class="n">df</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">}))</span> <span class="o">-</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="p">})</span>

<span class="n">pred_elasticity</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_elast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2648</th>
      <td>18.6</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>185</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>2456</th>
      <td>26.0</td>
      <td>3</td>
      <td>0.5</td>
      <td>10</td>
      <td>200</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>4557</th>
      <td>23.7</td>
      <td>3</td>
      <td>0.3</td>
      <td>8</td>
      <td>192</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>4884</th>
      <td>28.9</td>
      <td>4</td>
      <td>1.5</td>
      <td>6</td>
      <td>213</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>92</th>
      <td>23.7</td>
      <td>1</td>
      <td>0.5</td>
      <td>8</td>
      <td>207</td>
      <td>-2.751463</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Making elasticity predictions using <span class="math notranslate nohighlight">\(m1\)</span> is not much fun. We can see that it predicts the exact same value for all the days. That’s because there are no interaction terms on that model. However, if we make predictions using <span class="math notranslate nohighlight">\(m3\)</span>, it outputs a different elasticity prediction for each day. That’s because now, the elasticity or treatment effect depend on the day specific features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_elast3</span> <span class="o">=</span> <span class="n">pred_elasticity</span><span class="p">(</span><span class="n">m3</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pred_elast3</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_elast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4764</th>
      <td>31.1</td>
      <td>6</td>
      <td>1.0</td>
      <td>3</td>
      <td>212</td>
      <td>1.144309</td>
    </tr>
    <tr>
      <th>4324</th>
      <td>24.8</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>182</td>
      <td>-9.994303</td>
    </tr>
    <tr>
      <th>4536</th>
      <td>25.0</td>
      <td>2</td>
      <td>1.5</td>
      <td>6</td>
      <td>205</td>
      <td>0.279273</td>
    </tr>
    <tr>
      <th>3466</th>
      <td>26.0</td>
      <td>3</td>
      <td>1.5</td>
      <td>3</td>
      <td>205</td>
      <td>0.308320</td>
    </tr>
    <tr>
      <th>115</th>
      <td>19.3</td>
      <td>3</td>
      <td>0.3</td>
      <td>9</td>
      <td>177</td>
      <td>-0.349745</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Notice how the predictions are numbers that go from something like -9 to something 1. Those are not predictions of the sales column, which is in the order of the hundreds. Rather, <strong>it’s a prediction of how much sales would change if we increased price by one unit</strong>. Right out of the bet, we can see some strange numbers. For example, take a look at day 4764. It’s predicting a positive elasticity. In other words, we are predicting that sales will increase if we increase the price of ice cream. This doesn’t appeal to our economic sense. It’s probably the case that the model is doing some weird extrapolation on that prediction. Fortunately, you don’t have to worry too much about it. Remember that our ultimate goal is to segment the units by how sensitive they are to the treatment. It’s <strong>not</strong> to come up with the most accurate elasticity prediction ever. For our main goal, it suffices if the elasticity predictions orders the units according to how sensitive they are. In other words, even if positive elasticity predictions like 1.1, or 0.5 don’t make much sense, all we need is that the ordering is correct, that is, we want the units with prediction 1.1 to be less impacted by price increase than units with predictions 0.5.</p>
<p>Ok, we have our elasticity or CATE model. But there is still a lurking question: how do they compare to a ML predictive model? Let’s try that now. We will use a machine learning algorithm that uses price, temperature, weekday and cost as features <span class="math notranslate nohighlight">\(X\)</span> and tries to predict ice cream sales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;temp&quot;</span><span class="p">,</span> <span class="s2">&quot;weekday&quot;</span><span class="p">,</span> <span class="s2">&quot;cost&quot;</span><span class="p">,</span> <span class="s2">&quot;price&quot;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;sales&quot;</span>
<span class="n">ml</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">()</span>
<span class="n">ml</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">X</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>

<span class="c1"># make sure the model is not overfiting.</span>
<span class="n">ml</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">X</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9124088322890126
</pre></div>
</div>
</div>
</div>
<p>This model can make predictions about how much sales we will have each day. But is it suited for what we really want? In other words, can this model distinguish between days where people are more sensitive to ice cream prices? Can it help us decide how much to charge depending on that price sensitivity?</p>
<p>To see which model is more useful, let’s try using them for segmenting the units. For each model, we will partition the units into 2 groups. Our hope is that one group is highly responsive to price increase while the other not so much. If that is the case, we can organize our business around those groups: for the days that fall in the high responsiveness group, we better not set prices too high. For the low responsiveness group, we can increase prices without risking too much in sales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bands_df</span> <span class="o">=</span> <span class="n">pred_elast3</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">elast_band</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">pred_elast3</span><span class="p">[</span><span class="s2">&quot;pred_elast&quot;</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span> <span class="c1"># create two groups based on elasticity predictions </span>
    <span class="n">pred_sales</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_elast3</span><span class="p">[</span><span class="n">X</span><span class="p">]),</span>
    <span class="n">pred_band</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">ml</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_elast3</span><span class="p">[</span><span class="n">X</span><span class="p">]),</span> <span class="mi">2</span><span class="p">),</span> <span class="c1"># create two groups based on sales predictions</span>
<span class="p">)</span>

<span class="n">bands_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_elast</th>
      <th>elast_band</th>
      <th>pred_sales</th>
      <th>pred_band</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2648</th>
      <td>18.6</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>185</td>
      <td>-10.301045</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>186.878081</td>
      <td>(161.089, 198.735]</td>
    </tr>
    <tr>
      <th>2456</th>
      <td>26.0</td>
      <td>3</td>
      <td>0.5</td>
      <td>10</td>
      <td>200</td>
      <td>0.036165</td>
      <td>(-0.00555, 1.389]</td>
      <td>203.188327</td>
      <td>(198.735, 257.746]</td>
    </tr>
    <tr>
      <th>4557</th>
      <td>23.7</td>
      <td>3</td>
      <td>0.3</td>
      <td>8</td>
      <td>192</td>
      <td>-0.132057</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>188.800637</td>
      <td>(161.089, 198.735]</td>
    </tr>
    <tr>
      <th>4884</th>
      <td>28.9</td>
      <td>4</td>
      <td>1.5</td>
      <td>6</td>
      <td>213</td>
      <td>0.860663</td>
      <td>(-0.00555, 1.389]</td>
      <td>210.430813</td>
      <td>(198.735, 257.746]</td>
    </tr>
    <tr>
      <th>92</th>
      <td>23.7</td>
      <td>1</td>
      <td>0.5</td>
      <td>8</td>
      <td>207</td>
      <td>-9.953698</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>209.044522</td>
      <td>(198.735, 257.746]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Next, we need to compare which of these two segmentations is the best one. I might be getting ahead of myself now, since we will only look at CATE model evaluation in the next chapter. But I feel I can give you a taste of what it looks like. One very simple way to check how good are those partition schemas - and by good I mean useful - is to plot a regression line of prices on sales for each partition. We can achieve that easily with Seaborn’s <code class="docutils literal notranslate"><span class="pre">regplot</span></code> combined with <code class="docutils literal notranslate"><span class="pre">FacetGrid</span></code>.</p>
<p>Below, we can see the partitions made using the elasticity predictions. Remember that all of this is done in the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">bands_df</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;elast_band&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sales&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_titles</span><span class="p">(</span><span class="n">col_template</span><span class="o">=</span><span class="s2">&quot;Elast. Band </span><span class="si">{col_name}</span><span class="s2">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_22_0.png" src="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_22_0.png" />
</div>
</div>
<p>As we can see, it looks like this partitioning scheme is useful. For the first partition, there is a high price sensitivity. Sales are going down by a lot as prices go up. However, for the second partition, sales remain roughly unchanged as price goes up. In fact, it even looks like sales are going up as we increase price, but that’s probably noise.</p>
<p>Contrast this with the partitions made using the ML prediction model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">bands_df</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;pred_band&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sales&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_titles</span><span class="p">(</span><span class="n">col_template</span><span class="o">=</span><span class="s2">&quot;Pred. Band </span><span class="si">{col_name}</span><span class="s2">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_24_0.png" src="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_24_0.png" />
</div>
</div>
<p>I really like this plot because it conveys a very important point. As you can see, the predictive model partitions are splitting the units on the y axis. On days like those in the first partition, we don’t sell a lot of ice cream, but we do sell more on days like those in the second partition. I find this amazing because the prediction model is doing exactly what it is supposed to do: it predicts sales. It can distinguish between days where there will be low versus high ice cream sales.</p>
<p>The only problem is that prediction is not particularly useful here. Ultimately, we want to know when we can increase prices and when we can’t. But once we look at the slopes of the lines in the predictive model partitions, we see that they don’t change much. In other words, both partitions, as defined by the prediction model, have about the same responsiveness to price increase. This doesn’t offer us much insight into which are the days we can increase prices, since it looks like price is not affecting sales at all.</p>
</div>
<div class="section" id="key-ideas">
<h2>Key Ideas<a class="headerlink" href="#key-ideas" title="Permalink to this headline">¶</a></h2>
<p>We finally formalized the concept of Conditional Average Treatment Effect and how it can be useful for personalisation. Namely, if we can understand how each unit responds to a treatment, that is, if we can understand the heterogeneity of the treatment effect, we can give the best treatment depending on the unit’s individual characteristics.</p>
<p>We’ve also contrasted this goal with that of a predictive model. Namely, we are rethinking the estimation task, from predicting <span class="math notranslate nohighlight">\(Y\)</span> in it’s raw format to predicting how <span class="math notranslate nohighlight">\(Y\)</span> changes with <span class="math notranslate nohighlight">\(T\)</span>, <span class="math notranslate nohighlight">\(\frac{\delta y}{\delta t}\)</span>.</p>
<p>Sadly, it’s not at all obvious how to build models for that. Since we can’t observe elasticity directly, it’s hard to make a model that predicts it. But linear regression came to our rescue. By using a regression model that is fitted to predict <span class="math notranslate nohighlight">\(Y\)</span>, we found a way to also predict <span class="math notranslate nohighlight">\(\frac{\delta y}{\delta t}\)</span>. We also had to include interaction terms of the treatment and the features. This made it so that our elasticity prediction was different for each customer. In other words, we were now estimating <span class="math notranslate nohighlight">\(E[T'(t) | X]\)</span>. Those elasticity predictions were then used to group our units into more or less sensitive to the treatment, ultimately helping us decide the treatment level for each group.</p>
<p><img alt="img" src="_images/economists.png" /></p>
<p>One natural question that arises from all this is if we can replace the linear regression by a generic machine learning model and use that to predict elasticity. The answer is yes, but there are some caveats. This chapter used a very simple CATE model because I think it’s easier to understand the concept behind them with linear regression. Don’t worry, though. We will see some more sophisticated models in the chapters to come. But before that, I first need to cover a very important topic, which is how can we compare two CATE models and decide which one is better.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>The things I’ve written here are mostly stuff from my head. I’ve learned them through experience. This means there isn’t a direct reference I can point you to. It also means that the things I wrote here have <strong>not</strong> passed the academic scrutiny that good science often goes through. Instead, notice how I’m talking about things that work in practice, but I don’t spend too much time explaining why that is the case. It’s a sort of science from the streets, if you will. However, I am putting this up for public scrutiny, so, by all means, if you find something preposterous, open an issue and I’ll address it to the best of my efforts.</p>
</div>
<div class="section" id="contribute">
<h2>Contribute<a class="headerlink" href="#contribute" title="Permalink to this headline">¶</a></h2>
<p>Causal Inference for the Brave and True is an open-source material on causal inference, the statistics of science. It uses only free software, based in Python. Its goal is to be accessible monetarily and intellectually.
If you found this book valuable and you want to support it, please go to <a class="reference external" href="https://www.patreon.com/causal_inference_for_the_brave_and_true">Patreon</a>. If you are not ready to contribute financially, you can also help by fixing typos, suggesting edits or giving feedback on passages you didn’t understand. Just go to the book’s repository and <a class="reference external" href="https://github.com/matheusfacure/python-causality-handbook/issues">open an issue</a>. Finally, if you liked this content, please share it with others who might find it useful and give it a <a class="reference external" href="https://github.com/matheusfacure/python-causality-handbook/stargazers">star on GitHub</a>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "causal-glory"
        },
        kernelOptions: {
            kernelName: "causal-glory",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'causal-glory'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="17-Predictive-Models-101.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">17 - Predictive Models 101</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="19-Evaluating-Causal-Models.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">19 - Evaluating Causal Models</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Matheus Facure Alves<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-97848161-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>