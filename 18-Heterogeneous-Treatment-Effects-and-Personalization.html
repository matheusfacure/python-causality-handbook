
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>18 - Heterogeneous Treatment Effects and Personalization &#8212; Causal Inference for the Brave and True</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="19 - Evaluating Causal Models" href="19-Evaluating-Causal-Models.html" />
    <link rel="prev" title="17 - Predictive Models 101" href="17-Predictive-Models-101.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-97848161-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Causal Inference for the Brave and True</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="landing-page.html">
                    Causal Inference for The Brave and True
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part I - The Yang
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Introduction-To-Causality.html">
   01 - Introduction To Causality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-Randomised-Experiments.html">
   02 - Randomised Experiments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-Stats-Review-The-Most-Dangerous-Equation.html">
   03 - Stats Review: The Most Dangerous Equation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-Graphical-Causal-Models.html">
   04 - Graphical Causal Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-The-Unreasonable-Effectiveness-of-Linear-Regression.html">
   05 - The Unreasonable Effectiveness of Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Grouped-and-Dummy-Regression.html">
   06 - Grouped and Dummy Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-Beyond-Confounders.html">
   07 - Beyond Confounders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-Instrumental-Variables.html">
   08 - Instrumental Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-Non-Compliance-and-LATE.html">
   09 - Non Compliance and LATE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Matching.html">
   10 - Matching
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11-Propensity-Score.html">
   11 - Propensity Score
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12-Doubly-Robust-Estimation.html">
   12 - Doubly Robust Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13-Difference-in-Differences.html">
   13 - Difference-in-Differences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14-Panel-Data-and-Fixed-Effects.html">
   14 - Panel Data and Fixed Effects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15-Synthetic-Control.html">
   15 - Synthetic Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16-Regression-Discontinuity-Design.html">
   16 - Regression Discontinuity Design
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part II - The Yin
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="17-Predictive-Models-101.html">
   17 - Predictive Models 101
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   18 - Heterogeneous Treatment Effects and Personalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19-Evaluating-Causal-Models.html">
   19 - Evaluating Causal Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20-Plug-and-Play-Estimators.html">
   20 - Plug-and-Play Estimators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21-Meta-Learners.html">
   21 - Meta Learners
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="22-Debiased-Orthogonal-Machine-Learning.html">
   22 - Debiased/Orthogonal Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="23-Challenges-with-Effect-Heterogeneity-and-Nonlinearity.html">
   23 - Challenges with Effect Heterogeneity and Nonlinearity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="24-The-Diff-in-Diff-Saga.html">
   24 - The Difference-in-Differences Saga
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="25-Synthetic-Diff-in-Diff.html">
   25 - Synthetic Difference-in-Differences
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Debiasing-with-Orthogonalization.html">
   Debiasing with Orthogonalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Debiasing-with-Propensity-Score.html">
   Debiasing with Propensity Score
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="When-Prediction-Fails.html">
   When Prediction Fails
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Prediction-Metrics-For-Causal-Models.html">
   Why Prediction Metrics are Dangerous For Causal Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Conformal-Inference-for-Synthetic-Control.html">
   Conformal Inference for Synthetic Controls
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contribute
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://www.patreon.com/causal_inference_for_the_brave_and_true">
   Patreon
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/matheusfacure/python-causality-handbook/master?urlpath=tree/18-Heterogeneous-Treatment-Effects-and-Personalization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/matheusfacure/python-causality-handbook/issues/new?title=Issue%20on%20page%20%2F18-Heterogeneous-Treatment-Effects-and-Personalization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/18-Heterogeneous-Treatment-Effects-and-Personalization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-predictions-to-causal-inference">
   From Predictions to Causal Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-ate-to-cate">
   From ATE to CATE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predicting-sensitivity">
   Predicting Sensitivity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-ideas">
   Key Ideas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contribute">
   Contribute
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>18 - Heterogeneous Treatment Effects and Personalization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-predictions-to-causal-inference">
   From Predictions to Causal Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-ate-to-cate">
   From ATE to CATE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predicting-sensitivity">
   Predicting Sensitivity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-ideas">
   Key Ideas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contribute">
   Contribute
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="heterogeneous-treatment-effects-and-personalization">
<h1>18 - Heterogeneous Treatment Effects and Personalization<a class="headerlink" href="#heterogeneous-treatment-effects-and-personalization" title="Permalink to this headline">#</a></h1>
<section id="from-predictions-to-causal-inference">
<h2>From Predictions to Causal Inference<a class="headerlink" href="#from-predictions-to-causal-inference" title="Permalink to this headline">#</a></h2>
<p>In the last chapter, we briefly covered Machine Learning models. ML models are tools for what I called predictions or, more technically, estimating the conditional expectation function <span class="math notranslate nohighlight">\(E[Y|X]\)</span>. In other words, ML is incredibly useful when you want to map from a known input <span class="math notranslate nohighlight">\(X\)</span> (like an english sentence, sales this month, brain scan images) to an initially unknown but well-defined output <span class="math notranslate nohighlight">\(Y\)</span> (like japanese sentences, sales next month or cancer diagnostics). So, if ML deals with predictions, or estimating <span class="math notranslate nohighlight">\(E[Y|X]\)</span>, for it to be useful, you have to frame whatever problem you want to solve with ML, as a prediction problem, a problem where estimating <span class="math notranslate nohighlight">\(E[Y|X]\)</span> is the key. We walked through such an example in the last chapter. There, we had to predict customer profitability from customer specific features: <span class="math notranslate nohighlight">\(E[NetValue|Age, Income, Region]\)</span>. This information was very useful, because it allowed us to focus our effort on engaging with profitable customers, while not doing business with non-profitable customers. Here, predicting profitability well is key.</p>
<p>Notice that it is a passive approach to estimation in the sense that you remove yourself from the data generating process. In our example, we assumed that customer profitability, <code class="docutils literal notranslate"><span class="pre">NetValue</span></code>, was given. All we had to do was estimate it. In other words, we assumed there was nothing we could do about the customer’s profitability other than predict it. We could not increase it, nor decrease it. But that is not always true. In fact, a lot of times, companies have levers they can use to increase customer profitability. These levers can range from a prime or cheaper customer service, to discount, prices or marketing. In the industry, it’s often the case that we are inserted in the data generating process. We can affect it. Hence, as data scientists working in the industry, we often have to answer what is the best course of action or what intervention to make in order to optimise some business metric, usually profitability or some other intermediary metric like conversion, costs or sales.</p>
<p>In this world, where we are not passive observers, estimating <span class="math notranslate nohighlight">\(E[Y|X]\)</span> is not the full picture. Here is where we enter causal inference. We need to add another piece to our conditional expectation function. That piece is precisely what models our participation in the data generating process. This piece is the treatment:</p>
<div class="math notranslate nohighlight">
\[
E[Y|X, T]
\]</div>
<p>We now have to make the distinction between context or exogenous features <span class="math notranslate nohighlight">\(X\)</span> and treatments <span class="math notranslate nohighlight">\(T\)</span>. Both impact the outcome <span class="math notranslate nohighlight">\(Y\)</span>, but while we have no control over <span class="math notranslate nohighlight">\(X\)</span>, we can decide what value <span class="math notranslate nohighlight">\(T\)</span> will take or at least intervene on it. To give a concrete example, <span class="math notranslate nohighlight">\(Y\)</span> could be sales in a day, <span class="math notranslate nohighlight">\(X\)</span> could be context features you can’t control, but that gives you information about sales, like average sales in the previous days, and <span class="math notranslate nohighlight">\(T\)</span> is the treatment variable you can intervene on in order to increase sales, like price, item stock levels or marketing. Causal inference is then the process of estimating the causal relationship between <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> under context <span class="math notranslate nohighlight">\(X\)</span>. Once we’ve done that, optimizing <span class="math notranslate nohighlight">\(Y\)</span> is just a matter of setting the treatment <span class="math notranslate nohighlight">\(T\)</span> in an optimal way</p>
<div class="math notranslate nohighlight">
\[
\underset{T}{argmax} \ E[Y|X, T]
\]</div>
<p>In that sense, beyond the positive aspect of causal inference, we also have a normative motivation.</p>
<p>In Part I, we tried to answer questions like, what is the value of schooling? Can law changes decrease smoking levels? Can we increase academic achievement by having a positive mindset? What is the impact of alcohol on mortality rates? All those questions are interesting from the purely scientific view of understanding how the world works. But there is also practical motivation behind them. If we know the impact of schooling on earnings, we can understand what is a reasonable price to pay for it. In math terms, what we are doing is estimating the causal inference of schooling and optimizing it: <span class="math notranslate nohighlight">\(\underset{Educ}{argmax} \ E[Income|X, Educ]\)</span>.</p>
<p>Part I was focused on answering if a treatment was overall positive, strong or zero. For example, we wanted to know if investing in education was, in general, a good idea or not. Also in Part I, the role of <span class="math notranslate nohighlight">\(X\)</span> was twofold. First, <span class="math notranslate nohighlight">\(X\)</span> could contain confounders, in which case, the causal effect was only identifiable if we accounted for or adjusted for <span class="math notranslate nohighlight">\(X\)</span>. Or, <span class="math notranslate nohighlight">\(X\)</span> could act to reduce the variance of the causal estimation. If <span class="math notranslate nohighlight">\(X\)</span> is a good predictor of <span class="math notranslate nohighlight">\(Y\)</span>, we can use it to explain away variance in <span class="math notranslate nohighlight">\(Y\)</span>, making the causal effect more apparent.</p>
<p>Now, things will become less black and white. We want more than just the average treatment effect. We will allow the treatment to impact positively some people but not others. Context features <span class="math notranslate nohighlight">\(X\)</span> will play a role in defining different profiles of units, and each profile might respond differently to the treatment. We now want to personalize the treatment, giving it only to those units that best respond to it. We are going from a world where all we cared about was the average treatment effect to one where we want the heterogeneous treatment effect.</p>
</section>
<section id="from-ate-to-cate">
<h2>From ATE to CATE<a class="headerlink" href="#from-ate-to-cate" title="Permalink to this headline">#</a></h2>
<p>So far, every time we’ve estimated the causal impact of a treatment, it was the average treatment effect (or, sometimes, the local average treatment effect):</p>
<div class="math notranslate nohighlight">
\[
E[Y_1−Y_0]
\]</div>
<p>or the continuous treatment equivalent</p>
<div class="math notranslate nohighlight">
\[
E[y'(t)]
\]</div>
<p>where <span class="math notranslate nohighlight">\(y'(t)\)</span> is the treatment derivative of the response function or outcome. We’ve learned techniques to uncover the general effectiveness of a treatment. ATE estimation is the bedrock of causal inference. It’s a super useful tool for the decision making problem we refer to as program evaluation. We want to know if we should roll out a treatment to the entire population or not. Don’t be confused by the public policy terms. The same technique to estimate the effectiveness of a national education or health program can also be used to know the effect of launching a new product on a company’s bottom line. The key thing to note here is that the decision we want to inform is if we should treat or not.</p>
<p>Now, we will try to inform another type of decision: <strong>who</strong> do we treat? Now, we allow the decision to change from one unit to another. It might be beneficial to treat one unit but not another. We want to personalize the treatment. In more technical terms, we want to estimate the Conditional Average Treatment Effect (CATE)</p>
<div class="math notranslate nohighlight">
\[
E[Y_1−Y_0 | X] \ \text{or} \ E[y'(t)|X]
\]</div>
<p>The conditioning on <span class="math notranslate nohighlight">\(X\)</span> means that we now allow the treatment effect to be different depending on the characteristics of each unit. Again, here, we believe that not all entities respond equally well to the treatment. We want to leverage that heterogeneity. We want to treat only the right units (in the binary case) or figure out what is the optimal treatment dosage for each unit (in the continuous case).</p>
<p>For instance, if you are a bank that has to decide the loan each customer is eligible for, you can be damn sure that it’s not a good idea to give loads of money to everyone - although it might be reasonable for some. You will have to be smart with your treatment (loan amount). Perhaps, depending on the customer’s credit score (<span class="math notranslate nohighlight">\(X\)</span>), you can figure out what is the proper loan dosage. Of course, you don’t need to be a big institution to leverage personalization. There’s no shortage of examples where it applies. What days of the year should you do sales? How much should you charge for whatever product? How much exercise is too much exercise for each person?</p>
<p>Think of it this way. You have a bunch of customers and a treatment (price, discount, loan,…). You want to personalize the treatment, for example, give different discounts to different customers.</p>
<p><img alt="img" src="_images/customers.png" /></p>
<p>To do that, you have to segment your customers. You have created groups that respond differently to your treatment. For example, you want to find customers that respond well to discounts and customers who respond poorly to it. Well, the customer’s response to a treatment is given by the conditional treatment effect <span class="math notranslate nohighlight">\(\frac{\delta Y}{ \delta T}\)</span>. So, we could somehow estimate that for each customer, we could group together those that respond great to the treatment (high treatment effect) and those that don’t respond very well to it. If we did that, we would split the customers space somewhat like the following image.</p>
<p><img alt="img" src="_images/elast-partition.png" /></p>
<p>That would be wonderful because now we would be able to estimate different treatment effects or sensitivities on each partition. And notice that the sensitivity is just the slope of the line or function that goes from <span class="math notranslate nohighlight">\(T\)</span> to <span class="math notranslate nohighlight">\(Y\)</span>. So, if we can produce partitions where the slope or sensitivity differs, it means that entities on those partitions have different responsiveness to the treatment.</p>
<p><img alt="img" src="_images/elast-split.png" /></p>
<p>In other words, what you want is to move away from predicting <span class="math notranslate nohighlight">\(Y\)</span> in its raw form and start to predict the derivative of <span class="math notranslate nohighlight">\(Y\)</span> on <span class="math notranslate nohighlight">\(T\)</span>,  <span class="math notranslate nohighlight">\(\frac{\delta Y}{ \delta T}\)</span> for each unit. For example, suppose that <span class="math notranslate nohighlight">\(Y\)</span> is ice cream sales, <span class="math notranslate nohighlight">\(T\)</span> is ice cream price and each unit <span class="math notranslate nohighlight">\(i\)</span> is a day. Let’s set moral issues aside, for the sake of argument, and pretend that you can change the price of ice cream every day. If you can somehow find the days where <span class="math notranslate nohighlight">\(\frac{\delta Sales}{ \delta Price}\)</span> <strong>is low</strong>, you can <strong>increase prices</strong> without losing many sales on those days. Perhaps you do this already, say, when you increase them during the holiday season. The point being, it’s useful to differentiate days in terms of the price sensitivity because it gives you some basis on how to set prices in an optimal way.</p>
<p>Ok, you might say, but this is kind of tricky. How can I predict sensitivity <span class="math notranslate nohighlight">\(\frac{\delta Sales}{ \delta Price}\)</span> if I can’t see it? That’s a very good point. sensitivity is essentially non observable on a unit level. Not only that, it’s a strange concept. We are much more accustomed to thinking in terms of raw quantities rather than in terms of change rates of those same quantities. So, to conceptualize sensitivity better, here is a little trick. You can think about each entity as having a <span class="math notranslate nohighlight">\(Y_i\)</span> value, sales in our example, but also an individual sensitivity <span class="math notranslate nohighlight">\(\frac{\delta Y_i}{\delta T_i}\)</span>. The sensitivity is how much <span class="math notranslate nohighlight">\(Y\)</span> changes with <span class="math notranslate nohighlight">\(T\)</span>, so you can think about each entity also having a slope coefficient associated with it <span class="math notranslate nohighlight">\(\frac{\delta Y}{ \delta T}_i\)</span>. In our example, we would say each day has a slope coefficient of price on sales.</p>
<p><img alt="img" src="_images/elasticity.png" /></p>
<p>Of course, we can’t see those individual slope coefficients. For us to see the individual slopes, we would have to observe each day under two different prices and calculate how the sales changes for each of those prices.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta Y_i}{ \delta T_i} \approx \frac{Y(T_i) - Y(T_i + \epsilon)}{T_i - (T_i + \epsilon)}
\]</div>
<p>This is the fundamental problem of causal inference all over again. We can’t ever see the same unit under different treatment conditions. So, what can we do?</p>
</section>
<section id="predicting-sensitivity">
<h2>Predicting Sensitivity<a class="headerlink" href="#predicting-sensitivity" title="Permalink to this headline">#</a></h2>
<p>We got ourselves into a complicated situation here. We’ve agreed that we need to predict <span class="math notranslate nohighlight">\(\frac{\delta Y_i}{ \delta T_i}\)</span>, which is sadly not observable. So it’s not like we could use a ML algorithm and plug that as it’s target. But maybe we don’t need to observe <span class="math notranslate nohighlight">\(\frac{\delta Y_i}{ \delta T_i}\)</span> in order to predict it</p>
<p>Here is an idea. What if we use linear regression?</p>
<p><img alt="img" src="_images/linear-fix.png" /></p>
<p>Let’s say you fit the following linear model to your data.</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 t_i + \beta_2 X_i + e_i
\]</div>
<p>If you differentiate it on the treatment, you will end up with</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y_i}{\delta t_i} = \beta_1 
\]</div>
<p>And since you can estimate the model above to get <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, we might even be as bold as to say that <strong>you can predict sensitivity even though you can’t observe it</strong>. In the case above, it is a rather simple prediction, that is, we are predicting the constant value <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> for everyone. That’s something, but not yet what we want. That’s the ATE, not the CATE. This doesn’t help us in our task of grouping entities according to how responsive they are to the treatment, simply because everyone gets the same sensitivity prediction. However, we can do the following simple change</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 t_i + \beta_2 X_i + \beta_3 t_i X_i  + e_i
\]</div>
<p>Which would in turn give us the following sensitivity prediction</p>
<div class="math notranslate nohighlight">
\[
\widehat{\frac{\delta y_i}{\delta t_i}} = \hat{\beta_1} + \hat{\beta_3}X_i
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\beta_3\)</span> is a vector coefficient for the features in <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Now each entity defined by a different <span class="math notranslate nohighlight">\(X_i\)</span> will have a different sensitivity prediction. In other words, the sensitivity prediction will change as <span class="math notranslate nohighlight">\(X\)</span> changes. Alas, regression can give us a way of estimating the CATE <span class="math notranslate nohighlight">\(E[y'(t)|X]\)</span>.</p>
<p>We are finally getting somewhere. The model above allows us to make a sensitivity prediction for each of our entities. With those predictions we can make more useful groups. We can take the units with high predicted sensitivity and group them together. We can do the same with the ones that have low predicted sensitivity. Finally, with our sensitivity predictions, we can group entities by how much we think they will respond to the treatment.</p>
<p>Enough of theory for now. It’s time to walk through an example of how to make this sort of sensitivity model. Let’s consider our ice cream example. Each unit <span class="math notranslate nohighlight">\(i\)</span> is a day. For each day, we know if it’s a weekday or not, what was the cost we had to make the ice cream (you can think of the cost as a proxy for quality) and the average temperature for that day. Those will be our feature space <span class="math notranslate nohighlight">\(X\)</span>. Then, we have our treatment, price, and our outcome, the number of ice cream sold. For this example, we will consider that the treatment is randomized, just so that we don’t have to worry about bias for now.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prices_rnd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./data/ice_cream_sales_rnd.csv&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prices_rnd</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">prices_rnd</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 5)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25.8</td>
      <td>1</td>
      <td>0.3</td>
      <td>7</td>
      <td>230</td>
    </tr>
    <tr>
      <th>1</th>
      <td>22.7</td>
      <td>3</td>
      <td>0.5</td>
      <td>4</td>
      <td>190</td>
    </tr>
    <tr>
      <th>2</th>
      <td>33.7</td>
      <td>7</td>
      <td>1.0</td>
      <td>5</td>
      <td>237</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.0</td>
      <td>4</td>
      <td>0.5</td>
      <td>5</td>
      <td>193</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.4</td>
      <td>1</td>
      <td>1.0</td>
      <td>3</td>
      <td>252</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Remember our goal here: we need to decide when to charge more and when to charge less depending on the day specific features: <code class="docutils literal notranslate"><span class="pre">temp</span></code> <code class="docutils literal notranslate"><span class="pre">weekday</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>. If that’s the goal, the treatment effect heterogeneity model needs to be evaluated with respect to its usefulness in achieving this goal. We will get to that in a moment (and a lot more in the next chapter). For now, let’s just split the dataset into a training and testing set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">prices_rnd</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have our training data, we need to make a model which will allow us to distinguish days with high price sensitivity from days with low price sensitivity. Our approach to do that will be to simply predict price sensitivity. How exactly? First, let’s consider using the following linear model</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \pmb{\beta_2}X_i + e_i
\]</div>
<p>If we inspect the parameters of this model, we can see what our predicted sensitivity will look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price + temp+C(weekday)+cost&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">m1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>  186.7113</td> <td>    1.770</td> <td>  105.499</td> <td> 0.000</td> <td>  183.241</td> <td>  190.181</td>
</tr>
<tr>
  <th>C(weekday)[T.2]</th> <td>  -25.0512</td> <td>    0.924</td> <td>  -27.114</td> <td> 0.000</td> <td>  -26.863</td> <td>  -23.240</td>
</tr>
<tr>
  <th>C(weekday)[T.3]</th> <td>  -24.5834</td> <td>    0.901</td> <td>  -27.282</td> <td> 0.000</td> <td>  -26.350</td> <td>  -22.817</td>
</tr>
<tr>
  <th>C(weekday)[T.4]</th> <td>  -24.3807</td> <td>    0.897</td> <td>  -27.195</td> <td> 0.000</td> <td>  -26.138</td> <td>  -22.623</td>
</tr>
<tr>
  <th>C(weekday)[T.5]</th> <td>  -24.9036</td> <td>    0.894</td> <td>  -27.850</td> <td> 0.000</td> <td>  -26.657</td> <td>  -23.150</td>
</tr>
<tr>
  <th>C(weekday)[T.6]</th> <td>  -24.0921</td> <td>    0.903</td> <td>  -26.693</td> <td> 0.000</td> <td>  -25.862</td> <td>  -22.323</td>
</tr>
<tr>
  <th>C(weekday)[T.7]</th> <td>   -0.8635</td> <td>    0.888</td> <td>   -0.972</td> <td> 0.331</td> <td>   -2.605</td> <td>    0.878</td>
</tr>
<tr>
  <th>price</th>           <td>   -2.7515</td> <td>    0.106</td> <td>  -25.970</td> <td> 0.000</td> <td>   -2.959</td> <td>   -2.544</td>
</tr>
<tr>
  <th>temp</th>            <td>    1.9848</td> <td>    0.060</td> <td>   33.117</td> <td> 0.000</td> <td>    1.867</td> <td>    2.102</td>
</tr>
<tr>
  <th>cost</th>            <td>    4.4718</td> <td>    0.528</td> <td>    8.462</td> <td> 0.000</td> <td>    3.436</td> <td>    5.508</td>
</tr>
</table></div></div>
</div>
<p>For <span class="math notranslate nohighlight">\(m1\)</span>, the predicted price sensitivity <span class="math notranslate nohighlight">\(\widehat{\dfrac{\delta y_i}{\delta t_i}}\)</span> will be given by <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, which is -2.75, in our case. This means that for each additional BRL we charge for our ice cream, we should expect sales to go down by about 3 units.</p>
<p>Notice how this <span class="math notranslate nohighlight">\(m1\)</span> predicts the exact same sensitivity for everyone. Hence, it is not a very good model if we want to know on which days people are less sensitive to ice cream prices. It estimates the ATE when what we need here is the CATE. Remember that our goal is to partition the entities in such a way that we can personalize and optimize our treatment (price) for each individual partition. If every prediction is the same, there is no partitioning we can make. We are not distinguishing sensitive from non sensitive units. To correct for that, consider our second model:</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \beta_2 price_i * temp_i * + \pmb{\beta_3}X_i + e_i
\]</div>
<p>This second model includes an <strong>interaction term</strong> between price and temperature. This means that it allows the sensitivity to differ for different temperatures. What we are effectively saying here is that people are more or less sensitive to price increases depending on the temperature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m2</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price*temp + C(weekday) + cost&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">m2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>  192.4767</td> <td>    4.371</td> <td>   44.037</td> <td> 0.000</td> <td>  183.907</td> <td>  201.046</td>
</tr>
<tr>
  <th>C(weekday)[T.2]</th> <td>  -25.0805</td> <td>    0.924</td> <td>  -27.143</td> <td> 0.000</td> <td>  -26.892</td> <td>  -23.269</td>
</tr>
<tr>
  <th>C(weekday)[T.3]</th> <td>  -24.5871</td> <td>    0.901</td> <td>  -27.290</td> <td> 0.000</td> <td>  -26.354</td> <td>  -22.821</td>
</tr>
<tr>
  <th>C(weekday)[T.4]</th> <td>  -24.4225</td> <td>    0.897</td> <td>  -27.231</td> <td> 0.000</td> <td>  -26.181</td> <td>  -22.664</td>
</tr>
<tr>
  <th>C(weekday)[T.5]</th> <td>  -24.8953</td> <td>    0.894</td> <td>  -27.844</td> <td> 0.000</td> <td>  -26.648</td> <td>  -23.142</td>
</tr>
<tr>
  <th>C(weekday)[T.6]</th> <td>  -24.1269</td> <td>    0.903</td> <td>  -26.726</td> <td> 0.000</td> <td>  -25.897</td> <td>  -22.357</td>
</tr>
<tr>
  <th>C(weekday)[T.7]</th> <td>   -0.8581</td> <td>    0.888</td> <td>   -0.966</td> <td> 0.334</td> <td>   -2.599</td> <td>    0.883</td>
</tr>
<tr>
  <th>price</th>           <td>   -3.6299</td> <td>    0.618</td> <td>   -5.873</td> <td> 0.000</td> <td>   -4.842</td> <td>   -2.418</td>
</tr>
<tr>
  <th>temp</th>            <td>    1.7459</td> <td>    0.176</td> <td>    9.912</td> <td> 0.000</td> <td>    1.401</td> <td>    2.091</td>
</tr>
<tr>
  <th>price:temp</th>      <td>    0.0366</td> <td>    0.025</td> <td>    1.443</td> <td> 0.149</td> <td>   -0.013</td> <td>    0.086</td>
</tr>
<tr>
  <th>cost</th>            <td>    4.4558</td> <td>    0.529</td> <td>    8.431</td> <td> 0.000</td> <td>    3.420</td> <td>    5.492</td>
</tr>
</table></div></div>
</div>
<p>Once we estimate the model, the predicted sensitivity is given by</p>
<div class="math notranslate nohighlight">
\[
\widehat{\frac{\delta sales_i}{\delta price_i}} = \hat{\beta_1} + \hat{\beta_2}temp_i
\]</div>
<p>Notice that <span class="math notranslate nohighlight">\(\hat{\beta_2}\)</span> is positive 0,03 and the baseline sensitivity <span class="math notranslate nohighlight">\(\beta_1\)</span> (the sensitivity at <span class="math notranslate nohighlight">\(0C^o\)</span>) is -3.6. This means that, on average, as we increase price, sales go down, which makes sense. It also means that for each additional degree in temperature, people become less sensitive to price increases on ice cream (although not by much). For example, at <span class="math notranslate nohighlight">\(25C^o\)</span>, for each additional BRL we charge, our sales go down by 2.8 units <span class="math notranslate nohighlight">\((-3.6 + (0.03 * 25))\)</span>. But at  <span class="math notranslate nohighlight">\(35C^o\)</span>, for each additional BRL we charge, they go down only by 2.5 units <span class="math notranslate nohighlight">\((-3.6 + (0.03 * 35))\)</span>. This is also sort of intuitive. As the days get hotter and hotter, people are willing to pay more for ice cream.</p>
<p>We can go even further. The next model includes interaction terms on all the feature space. This means that sensitivity will change with temperature, day of the week and cost.</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \pmb{\beta_2 X_i}*price_i + \pmb{\beta_3}X_i + e_i
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m3</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price*cost + price*C(weekday) + price*temp&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>According to the above model, the unit level sensitivity, or CATE, would be given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta Sales}{\delta Price} = \beta_1 + \pmb{\beta_2 X_i}
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\beta_1\)</span> is the price coefficient and <span class="math notranslate nohighlight">\(\pmb{\beta_2}\)</span> is the vector for the interaction coefficients.</p>
<p>Finally, let’s see how to actually make those sensitivity predictions. One way is to extract the sensitivity parameters from the model and use the formula above. However, we will resort to a more general approximation. Since sensitivity is nothing more than the derivative of the outcome on treatment, we can use the definition of the derivative.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y}{\delta t} = \dfrac{y(t+\epsilon) - y(t)}{ (t + \epsilon) - t }
\]</div>
<p>with <span class="math notranslate nohighlight">\(\epsilon\)</span> going to zero. We can approximate this definition by replacing <span class="math notranslate nohighlight">\(\epsilon\)</span> by 1.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y}{\delta t} \approx \hat{y}(t+1) - \hat{y}(t)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}\)</span> is given by our model’s predictions. In words, I’ll make two predictions with my models: one passing the original data and another passing the original data but with the treatment incremented by one unit. The difference between those predictions is my CATE prediction.</p>
<p>Below, you can see a function for doing that. Since we’ve used the train set to estimate our model, we will now make predictions on the test set. First, let’s use our first, ATE model, <span class="math notranslate nohighlight">\(m1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pred_sensitivity</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
        <span class="s2">&quot;pred_sens&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">t</span><span class="p">:</span><span class="n">df</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">}))</span> <span class="o">-</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="p">})</span>

<span class="n">pred_sensitivity</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_sens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2648</th>
      <td>18.6</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>185</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>2456</th>
      <td>26.0</td>
      <td>3</td>
      <td>0.5</td>
      <td>10</td>
      <td>200</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>4557</th>
      <td>23.7</td>
      <td>3</td>
      <td>0.3</td>
      <td>8</td>
      <td>192</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>4884</th>
      <td>28.9</td>
      <td>4</td>
      <td>1.5</td>
      <td>6</td>
      <td>213</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>92</th>
      <td>23.7</td>
      <td>1</td>
      <td>0.5</td>
      <td>8</td>
      <td>207</td>
      <td>-2.751463</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Making sensitivity predictions using <span class="math notranslate nohighlight">\(m1\)</span> is not much fun. We can see that it predicts the exact same value for all the days. That’s because there are no interaction terms on that model. However, if we make predictions using <span class="math notranslate nohighlight">\(m3\)</span>, it outputs a different sensitivity prediction for each day. That’s because now, the sensitivity or treatment effect depends on the day specific features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_sens3</span> <span class="o">=</span> <span class="n">pred_sensitivity</span><span class="p">(</span><span class="n">m3</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pred_sens3</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_sens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4764</th>
      <td>31.1</td>
      <td>6</td>
      <td>1.0</td>
      <td>3</td>
      <td>212</td>
      <td>1.144309</td>
    </tr>
    <tr>
      <th>4324</th>
      <td>24.8</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>182</td>
      <td>-9.994303</td>
    </tr>
    <tr>
      <th>4536</th>
      <td>25.0</td>
      <td>2</td>
      <td>1.5</td>
      <td>6</td>
      <td>205</td>
      <td>0.279273</td>
    </tr>
    <tr>
      <th>3466</th>
      <td>26.0</td>
      <td>3</td>
      <td>1.5</td>
      <td>3</td>
      <td>205</td>
      <td>0.308320</td>
    </tr>
    <tr>
      <th>115</th>
      <td>19.3</td>
      <td>3</td>
      <td>0.3</td>
      <td>9</td>
      <td>177</td>
      <td>-0.349745</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Notice how the predictions are numbers that go from something like -9 to something 1. Those are not predictions of the sales column, which is in the order of the hundreds. Rather, <strong>it’s a prediction of how much sales would change if we increased price by one unit</strong>. Right off of the bet, we can see some strange numbers. For example, take a look at day 4764. It’s predicting a positive sensitivity. In other words, we are predicting that sales will increase if we increase the price of ice cream. This doesn’t appeal to our economic sense. It’s probably the case that the model is doing some weird extrapolation on that prediction. Fortunately, you don’t have to worry too much about it. Remember that our ultimate goal is to segment the units by how sensitive they are to the treatment. It’s <strong>not</strong> to come up with the most accurate sensitivity prediction ever. For our main goal, it suffices if the sensitivity predictions orders the units according to how sensitive they are. In other words, even if positive sensitivity predictions like 1.1, or 0.5 don’t make much sense, all we need is that the ordering is correct, that is, we want the units with prediction 1.1 to be less impacted by price increase than units with predictions 0.5.</p>
<p>Ok, we have our sensitivity or CATE model. But there is still a lurking question: how do they compare to a ML predictive model? Let’s try that now. We will use a machine learning algorithm that uses price, temperature, weekday and cost as features <span class="math notranslate nohighlight">\(X\)</span> and tries to predict ice cream sales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;temp&quot;</span><span class="p">,</span> <span class="s2">&quot;weekday&quot;</span><span class="p">,</span> <span class="s2">&quot;cost&quot;</span><span class="p">,</span> <span class="s2">&quot;price&quot;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;sales&quot;</span>
<span class="n">ml</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">()</span>
<span class="n">ml</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">X</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>

<span class="c1"># make sure the model is not overfiting.</span>
<span class="n">ml</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">X</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9124088322890126
</pre></div>
</div>
</div>
</div>
<p>This model can make predictions about how much sales we will have each day. But is it suited for what we really want? In other words, can this model distinguish between days where people are more sensitive to ice cream prices? Can it help us decide how much to charge depending on that price sensitivity?</p>
<p>To see which model is more useful, let’s try using them for segmenting the units. For each model, we will partition the units into 2 groups. Our hope is that one group is highly responsive to price increase while the other not so much. If that is the case, we can organize our business around those groups: for the days that fall in the high responsiveness group, we better not set prices too high. For the low responsiveness group, we can increase prices without risking too much in sales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bands_df</span> <span class="o">=</span> <span class="n">pred_sens3</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">sens_band</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">pred_sens3</span><span class="p">[</span><span class="s2">&quot;pred_sens&quot;</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span> <span class="c1"># create two groups based on sensitivity predictions </span>
    <span class="n">pred_sales</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_sens3</span><span class="p">[</span><span class="n">X</span><span class="p">]),</span>
    <span class="n">pred_band</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">ml</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_sens3</span><span class="p">[</span><span class="n">X</span><span class="p">]),</span> <span class="mi">2</span><span class="p">),</span> <span class="c1"># create two groups based on sales predictions</span>
<span class="p">)</span>

<span class="n">bands_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_sens</th>
      <th>sens_band</th>
      <th>pred_sales</th>
      <th>pred_band</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2648</th>
      <td>18.6</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>185</td>
      <td>-10.301045</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>186.878081</td>
      <td>(161.089, 198.735]</td>
    </tr>
    <tr>
      <th>2456</th>
      <td>26.0</td>
      <td>3</td>
      <td>0.5</td>
      <td>10</td>
      <td>200</td>
      <td>0.036165</td>
      <td>(-0.00555, 1.389]</td>
      <td>203.188327</td>
      <td>(198.735, 257.746]</td>
    </tr>
    <tr>
      <th>4557</th>
      <td>23.7</td>
      <td>3</td>
      <td>0.3</td>
      <td>8</td>
      <td>192</td>
      <td>-0.132057</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>188.800637</td>
      <td>(161.089, 198.735]</td>
    </tr>
    <tr>
      <th>4884</th>
      <td>28.9</td>
      <td>4</td>
      <td>1.5</td>
      <td>6</td>
      <td>213</td>
      <td>0.860663</td>
      <td>(-0.00555, 1.389]</td>
      <td>210.430813</td>
      <td>(198.735, 257.746]</td>
    </tr>
    <tr>
      <th>92</th>
      <td>23.7</td>
      <td>1</td>
      <td>0.5</td>
      <td>8</td>
      <td>207</td>
      <td>-9.953698</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>209.044522</td>
      <td>(198.735, 257.746]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Next, we need to compare which of these two segmentations is the best one. I might be getting ahead of myself now, since we will only look at CATE model evaluation in the next chapter. But I feel I can give you a taste of what it looks like. One very simple way to check how good are those partition schemas - and by good I mean useful - is to plot a regression line of prices on sales for each partition. We can achieve that easily with Seaborn’s <code class="docutils literal notranslate"><span class="pre">regplot</span></code> combined with <code class="docutils literal notranslate"><span class="pre">FacetGrid</span></code>.</p>
<p>Below, we can see the partitions made using the sensitivity predictions. Remember that all of this is done in the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">bands_df</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;sens_band&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sales&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_titles</span><span class="p">(</span><span class="n">col_template</span><span class="o">=</span><span class="s2">&quot;Sens. Band </span><span class="si">{col_name}</span><span class="s2">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_22_0.png" src="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_22_0.png" />
</div>
</div>
<p>As we can see, it looks like this partitioning scheme is useful. For the first partition, there is a high price sensitivity. Sales are going down by a lot as prices go up. However, for the second partition, sales remain roughly unchanged as the price goes up. In fact, it even looks like sales are going up as we increase the price, but that’s probably noise.</p>
<p>Contrast this with the partitions made using the ML prediction model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">bands_df</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;pred_band&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sales&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_titles</span><span class="p">(</span><span class="n">col_template</span><span class="o">=</span><span class="s2">&quot;Pred. Band </span><span class="si">{col_name}</span><span class="s2">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_24_0.png" src="_images/18-Heterogeneous-Treatment-Effects-and-Personalization_24_0.png" />
</div>
</div>
<p>I really like this plot because it conveys a very important point. As you can see, the predictive model partitions are splitting the units on the y axis. On days like those in the first partition, we don’t sell a lot of ice cream, but we do sell more on days like those in the second partition. I find this amazing because the prediction model is doing exactly what it is supposed to do: it predicts sales. It can distinguish between days where there will be low versus high ice cream sales.</p>
<p>The only problem is that prediction is not particularly useful here. Ultimately, we want to know when we can increase prices and when we can’t. But once we look at the slopes of the lines in the predictive model partitions, we see that they don’t change much. In other words, both partitions, as defined by the prediction model, have about the same responsiveness to price increase. This doesn’t offer us much insight into which are the days we can increase prices, since it looks like price is not affecting sales at all.</p>
</section>
<section id="key-ideas">
<h2>Key Ideas<a class="headerlink" href="#key-ideas" title="Permalink to this headline">#</a></h2>
<p>We finally formalized the concept of Conditional Average Treatment Effect and how it can be useful for personalisation. Namely, if we can understand how each unit responds to a treatment, that is, if we can understand the heterogeneity of the treatment effect, we can give the best treatment depending on the unit’s individual characteristics.</p>
<p>We’ve also contrasted this goal with that of a predictive model. Namely, we are rethinking the estimation task, from predicting <span class="math notranslate nohighlight">\(Y\)</span> in it’s raw format to predicting how <span class="math notranslate nohighlight">\(Y\)</span> changes with <span class="math notranslate nohighlight">\(T\)</span>, <span class="math notranslate nohighlight">\(\frac{\delta y}{\delta t}\)</span>.</p>
<p>Sadly, it’s not at all obvious how to build models for that. Since we can’t observe sensitivity directly, it’s hard to make a model that predicts it. But linear regression came to our rescue. By using a regression model that is fitted to predict <span class="math notranslate nohighlight">\(Y\)</span>, we found a way to also predict <span class="math notranslate nohighlight">\(\frac{\delta y}{\delta t}\)</span>. We also had to include interaction terms of the treatment and the features. This made it so that our sensitivity prediction was different for each customer. In other words, we were now estimating <span class="math notranslate nohighlight">\(E[Y'(t) | X]\)</span>. Those sensitivity predictions were then used to group our units into more or less sensitive to the treatment, ultimately helping us decide the treatment level for each group.</p>
<p><img alt="img" src="_images/economists.png" /></p>
<p>One natural question that arises from all this is if we can replace the linear regression by a generic machine learning model and use that to predict sensitivity. The answer is yes, but there are some caveats. This chapter used a very simple CATE model because I think it’s easier to understand the concept behind them with linear regression. Don’t worry, though. We will see some more sophisticated models in the chapters to come. But before that, I first need to cover a very important topic, which is how can we compare two CATE models and decide which one is better.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<p>The things I’ve written here are mostly stuff from my head. I’ve learned them through experience. This means there isn’t a direct reference I can point you to. It also means that the things I wrote here have <strong>not</strong> passed the academic scrutiny that good science often goes through. Instead, notice how I’m talking about things that work in practice, but I don’t spend too much time explaining why that is the case. It’s a sort of science from the streets, if you will. However, I am putting this up for public scrutiny, so, by all means, if you find something preposterous, open an issue and I’ll address it to the best of my efforts.</p>
</section>
<section id="contribute">
<h2>Contribute<a class="headerlink" href="#contribute" title="Permalink to this headline">#</a></h2>
<p>Causal Inference for the Brave and True is an open-source material on causal inference, the statistics of science. It uses only free software, based in Python. Its goal is to be accessible monetarily and intellectually.
If you found this book valuable and you want to support it, please go to <a class="reference external" href="https://www.patreon.com/causal_inference_for_the_brave_and_true">Patreon</a>. If you are not ready to contribute financially, you can also help by fixing typos, suggesting edits or giving feedback on passages you didn’t understand. Just go to the book’s repository and <a class="reference external" href="https://github.com/matheusfacure/python-causality-handbook/issues">open an issue</a>. Finally, if you liked this content, please share it with others who might find it useful and give it a <a class="reference external" href="https://github.com/matheusfacure/python-causality-handbook/stargazers">star on GitHub</a>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-py10-py"
        },
        kernelOptions: {
            kernelName: "conda-env-py10-py",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-py10-py'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="17-Predictive-Models-101.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">17 - Predictive Models 101</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="19-Evaluating-Causal-Models.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">19 - Evaluating Causal Models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Matheus Facure Alves<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>