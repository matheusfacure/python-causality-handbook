{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly Robust Estimation\n",
    "\n",
    "## Don't Put All your Eggs in One Basket\n",
    "\n",
    "We've learned how to use linear regression and propensity score weighting to estimate \\\\(E[Y|Y=1] - E[Y|Y=0] | X\\\\). But which one should we use and when? When in doubt, just use both! Doubly Robust Estimation is a way of combining propensity score and linear regression in a way you don't have to rely on any single one of them. \n",
    "\n",
    "To see how this works, let's consider the mindset experiment. It is a randomised study conducted in U.S. public high schools which aims at finding the impact of a growth mindset. The way it works is that students receive from the school a seminary to instil in them a growth mindset. Then, they follow up the students on college years to measure how well they performed academically. This measurement was compiled into an achievement score and standardised. The real data on this study is not publicly available in order to preserve students' privacy. However, we have a simulated dataset with the same statistical properties provided by [Athey and Wager](https://arxiv.org/pdf/1902.07409.pdf), so we will use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "pd.set_option(\"display.max_columns\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schoolid</th>\n",
       "      <th>intervention</th>\n",
       "      <th>achievement_score</th>\n",
       "      <th>...</th>\n",
       "      <th>school_ethnic_minority</th>\n",
       "      <th>school_poverty</th>\n",
       "      <th>school_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>1.480828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.515202</td>\n",
       "      <td>-0.169849</td>\n",
       "      <td>0.173954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.987277</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.310927</td>\n",
       "      <td>0.224077</td>\n",
       "      <td>-0.426757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9963</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.152340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875012</td>\n",
       "      <td>-0.724801</td>\n",
       "      <td>0.761781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.358336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315755</td>\n",
       "      <td>0.054586</td>\n",
       "      <td>1.862187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.360920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033161</td>\n",
       "      <td>-0.982274</td>\n",
       "      <td>1.591641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      schoolid  intervention  achievement_score  ...  school_ethnic_minority  \\\n",
       "259         73             1           1.480828  ...               -0.515202   \n",
       "3435        76             0          -0.987277  ...               -1.310927   \n",
       "9963         4             0          -0.152340  ...                0.875012   \n",
       "4488        67             0           0.358336  ...                0.315755   \n",
       "2637        16             1           1.360920  ...               -0.033161   \n",
       "\n",
       "      school_poverty  school_size  \n",
       "259        -0.169849     0.173954  \n",
       "3435        0.224077    -0.426757  \n",
       "9963       -0.724801     0.761781  \n",
       "4488        0.054586     1.862187  \n",
       "2637       -0.982274     1.591641  \n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/learning_mindset.csv\")\n",
    "data.sample(5, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the study was randomised, it doesn't seem to be the case that this data is free from confounding. One possible reason for this is that the treatment variable is measured by the student's receipt of the seminar. So, although the opportunity to participate was random, participation is not. We are dealing with a case of non-compliance here. One evidence of this is how the student's success expectation is correlated with the participation in the seminar. Students with higher self-reported high expectations are more likely to have joined the growth mindset seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success_expect\n",
       "1    0.271739\n",
       "2    0.265957\n",
       "3    0.294118\n",
       "4    0.271617\n",
       "5    0.311070\n",
       "6    0.354287\n",
       "7    0.362319\n",
       "Name: intervention, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"success_expect\")[\"intervention\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know by now, we could adjust for this using a linear regression or by estimating a propensity score model with a logistic regression. Before we do that, however, we need to convert the categorical variables to dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10391, 32)\n"
     ]
    }
   ],
   "source": [
    "categ = [\"ethnicity\", \"gender\", \"school_urbanicity\"]\n",
    "cont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"]\n",
    "\n",
    "data_with_categ = pd.concat([\n",
    "    data.drop(columns=categ), # dataset without the categorical features\n",
    "    pd.get_dummies(data[categ], columns=categ, drop_first=False)# dataset without categorical converted to dummies\n",
    "], axis=1)\n",
    "\n",
    "print(data_with_categ.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to understand how doubly robust estimation works.\n",
    "\n",
    "## Doubly Robust Estimation\n",
    "\n",
    "![img](./data/img/doubly-robust/double.png)\n",
    "\n",
    "Instead of deriving the estimator, I'll first show it to you and only then tell why it is awesome.\n",
    "\n",
    "$\n",
    "\\hat{ATE} = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg) - \\frac{1}{N}\\sum \\bigg( \\dfrac{(1-T_i)(Y_i - \\hat{\\mu_0}(X_i))}{1-\\hat{P}(X_i)} + \\hat{\\mu_0}(X_i) \\bigg)\n",
    "$\n",
    "\n",
    "where \\\\(\\hat{P}(x)\\\\) is an estimation of the propensity score (using logistic regression, for example), \\\\(\\hat{\\mu_1}(x)\\\\) is an estimation of \\\\(E[Y|X, T=1]\\\\) (using linear regression, for example), and \\\\(\\hat{\\mu_0}(x)\\\\) is an estimation of \\\\(E[Y|X, T=0]\\\\). As you might have already guessed, the first part of the doubly robust estimator estimates \\\\(E[Y_1]\\\\) and the second part estimates \\\\(E[Y_0]\\\\). Let's examine the first part, as all the intuition will also apply to the second part by analogy.\n",
    "\n",
    "Since I know that this formula is scary at first (but don't worry, you will see it is super simple), I will first show how to code this estimator. I have the feeling that some people are less frightened by code than by formulas. Let's see how this estimator works in practice, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def doubly_robust(df, X, T, Y):\n",
    "    ps = LogisticRegression(C=1e6).fit(df[X], df[T]).predict_proba(df[X])[:, 1]\n",
    "    mu0 = LinearRegression().fit(df.query(f\"{T}==0\")[X], df.query(f\"{T}==0\")[Y]).predict(df[X])\n",
    "    mu1 = LinearRegression().fit(df.query(f\"{T}==1\")[X], df.query(f\"{T}==1\")[Y]).predict(df[X])\n",
    "    return (\n",
    "        np.mean(df[T]*(df[Y] - mu1)/ps + mu1) -\n",
    "        np.mean((1-df[T])*(df[Y] - mu0)/(1-ps) + mu0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38822192386353527"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 'intervention'\n",
    "Y = 'achievement_score'\n",
    "X = data_with_categ.columns.drop(['schoolid', T, Y])\n",
    "\n",
    "doubly_robust(data_with_categ, X, T, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubly robust estimator is saying that we should expect individuals who attended the mindset seminar to be 0.388 standard deviations above their untreated fellows, in terms of achievements. Once again, we can use bootstrap to construct confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed # for parallel processing\n",
    "\n",
    "np.random.seed(88)\n",
    "# run 1000 bootstrap samples\n",
    "bootstrap_sample = 1000\n",
    "ates = Parallel(n_jobs=4)(delayed(doubly_robust)(data_with_categ.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates = np.array(ates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE 95% CI: (0.3536507259630512, 0.4197834129772669)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ATE 95% CI:\", (np.percentile(ates, 2.5), np.percentile(ates, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(ates, kde=False)\n",
    "plt.vlines(np.percentile(ates, 2.5), 0, 20, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(ates, 97.5), 0, 20, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.title(\"ATE Bootstrap Distribution\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got a taste of the doubly robust estimator, let's examine why it is so great. First, it is called doubly robust because it only requires one of the models, \\\\(\\hat{P}(x)\\\\) or \\\\(\\hat{\\mu}(x)\\\\), to be correctly specified. To see this, take the first part that estimates \\\\(E[Y_1]\\\\) and take a good look at it.\n",
    "\n",
    "$\n",
    "\\hat{E}[Y_1] = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg)\n",
    "$\n",
    "\n",
    "Assume that \\\\(\\hat{\\mu_1}(x)\\\\) is correct. If the propensity score model is wrong, we wouldn't need to worry. Because if \\\\(\\hat{\\mu_1}(x)\\\\) is correct, then \\\\(E[T_i(Y_i - \\hat{\\mu_1}(X_i))]=0\\\\). That is because the multiplication by \\\\(T_i\\\\) selects only the treated and the residual of \\\\(\\hat{\\mu_1}\\\\) on the treated have, by definition, mean zero. This causes the whole thing to reduce to \\\\(\\hat{\\mu_1}(X_i)\\\\), which is correctly estimated \\\\(E[Y_1]\\\\) by assumption. So, you see, that by being correct, \\\\(\\hat{\\mu_1}(X_i)\\\\) wipes out the relevance of the propensity score model. We can apply the same reasoning to understand the estimator of \\\\(E[Y_0]\\\\). \n",
    "\n",
    "But don't take my word for it. Let the code show you the way! In the following estimator, I've replaced the logistic regression that estimates the propensity score by a random uniform variable that goes from 0.1 to 0.9 (I don't want very small weights to blow up my propensity score variance). Since this is random, there is no way it is a good propensity score model, but we will see that the doubly robust estimator still manages to produce an estimation that is very close to when the propensity score was estimated with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def doubly_robust_wrong_ps(df, X, T, Y):\n",
    "    # wrong PS model\n",
    "    np.random.seed(654)\n",
    "    ps = np.random.uniform(0.1, 0.9, df.shape[0])\n",
    "    mu0 = LinearRegression().fit(df.query(f\"{T}==0\")[X], df.query(f\"{T}==0\")[Y]).predict(df[X])\n",
    "    mu1 = LinearRegression().fit(df.query(f\"{T}==1\")[X], df.query(f\"{T}==1\")[Y]).predict(df[X])\n",
    "    return (\n",
    "        np.mean(df[T]*(df[Y] - mu1)/ps + mu1) -\n",
    "        np.mean((1-df[T])*(df[Y] - mu0)/(1-ps) + mu0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37982453125218174"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubly_robust_wrong_ps(data_with_categ, X, T, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use bootstrap, we can see that the variance is slightly higher than when the propensity score was estimated with a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(88)\n",
    "parallel_fn = delayed(doubly_robust_wrong_ps)\n",
    "wrong_ps = Parallel(n_jobs=4)(parallel_fn(data_with_categ.sample(frac=1, replace=True), X, T, Y)\n",
    "                              for _ in range(bootstrap_sample))\n",
    "wrong_ps = np.array(wrong_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE 95% CI: (0.3536507259630512, 0.4197834129772669)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ATE 95% CI:\", (np.percentile(ates, 2.5), np.percentile(ates, 97.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers the case that the propensity model is wrong but the outcome model is correct. What about the other situation? Let's again take a good look at the first part of the estimator, but let's rearrange some terms\n",
    "\n",
    "$\n",
    "\\hat{E}[Y_1] = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg)\n",
    "$\n",
    "\n",
    "$\n",
    "\\hat{E}[Y_1] = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_iY_i}{\\hat{P}(X_i)} - \\dfrac{T_i\\hat{\\mu_1}(X_i)}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg)\n",
    "$\n",
    "\n",
    "$\n",
    "\\hat{E}[Y_1] = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_iY_i}{\\hat{P}(X_i)} - \\bigg(\\dfrac{T_i}{\\hat{P}(X_i)} - 1\\bigg) \\hat{\\mu_1}(X_i) \\bigg)\n",
    "$\n",
    "\n",
    "$\n",
    "\\hat{E}[Y_1] = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_iY_i}{\\hat{P}(X_i)} - \\bigg(\\dfrac{T_i - \\hat{P}(X_i)}{\\hat{P}(X_i)}\\bigg) \\hat{\\mu_1}(X_i) \\bigg)\n",
    "$\n",
    "\n",
    "Now, assume that the propensity score \\\\(\\hat{P}(X_i)\\\\) is correctly specified. In this case, \\\\(E[T_i - \\hat{P}(X_i)]=0\\\\), which wipes out the part dependent on \\\\(\\hat{\\mu_1}(X_i)\\\\). This makes the doubly robust estimator reduce to the propensity score weighting estimator \\\\(\\frac{T_iY_i}{\\hat{P}(X_i)}\\\\), which is correct by assumption. So, even if the \\\\(\\hat{\\mu_1}(X_i)\\\\) is wrong, the estimator will still be correct, provided that the propensity score is correctly specified.\n",
    "\n",
    "Once again, if you believe more in code than in formulas, here it is the practical verification. In the code below, I've replaced both regression models with a random normal variable. There is no doubt that \\\\(\\hat{\\mu}(X_i)\\\\) is **not correctly specified**. Still, we will see that doubly robust estimation still manages to recover the same \\\\(\\hat{ATE}\\\\) of about 0.38 that we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def doubly_robust_wrong_model(df, X, T, Y):\n",
    "    np.random.seed(654)\n",
    "    ps = LogisticRegression(C=1e6).fit(df[X], df[T]).predict_proba(df[X])[:, 1]\n",
    "    \n",
    "    # wrong mu(x) model\n",
    "    mu0 = np.random.normal(0, 1, df.shape[0])\n",
    "    mu1 = np.random.normal(0, 1, df.shape[0])\n",
    "    return (\n",
    "        np.mean(df[T]*(df[Y] - mu1)/ps + mu1) -\n",
    "        np.mean((1-df[T])*(df[Y] - mu0)/(1-ps) + mu0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3745648055762825"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubly_robust_wrong_model(data_with_categ, X, T, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One again, we can use bootstrap and see that the variance is just slightly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(88)\n",
    "parallel_fn = delayed(doubly_robust_wrong_model)\n",
    "wrong_mux = Parallel(n_jobs=4)(parallel_fn(data_with_categ.sample(frac=1, replace=True), X, T, Y)\n",
    "                               for _ in range(bootstrap_sample))\n",
    "wrong_mux = np.array(wrong_mux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE 95% CI: (0.3536507259630512, 0.4197834129772669)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ATE 95% CI:\", (np.percentile(ates, 2.5), np.percentile(ates, 97.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope I've convinced you about the power of doubly robust estimation. Its magic happens because in causal inference, there are two ways to remove bias from our causal estimates: you either model the treatment mechanism or the outcome mechanism. If either of these models are correct, you are good to go\n",
    "\n",
    "One caveat is that, in practice, it's very hard to model precisely either of those. More often, what ends up happening is that neither the propensity score nor the outcome model are 100% correct. They are both wrong, but in different ways. When this happens, it is not exactly settled [\\[1\\]](https://www.stat.cmu.edu/~ryantibs/journalclub/kang_2007.pdf) [\\[2\\]](https://arxiv.org/pdf/0804.2969.pdf) [\\[3\\]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798744/) if it's better to use a single model or doubly robust estimation. As for me, I still like using them because at least it gives me two possibilities of being correct. \n",
    "\n",
    "\n",
    "## Keys Ideias\n",
    "\n",
    "Here, we saw a simple way of combining linear regression with the propensity score to produce a doubly robust estimator. This estimator bears that name because it only requires one of the models to be correct. If the propensity score model is correct, we will be able to identify the causal effect even if the outcome model is wrong. On the flip side, if the outcome model is correct, we will also be able to identify the causal effect even if the propensity score model is wrong.\n",
    "\n",
    "## References\n",
    "\n",
    "I like to think of this entire book as a tribute to Joshua Angrist, Alberto Abadie and Christopher Walters for their amazing Econometrics class. Most of the ideas here are taken from their classes at the American Economic Association. Watching them is what is keeping me sane during this tough year of 2020.\n",
    "* [Cross-Section Econometrics](https://www.aeaweb.org/conference/cont-ed/2017-webcasts)\n",
    "* [Mastering Mostly Harmless Econometrics](https://www.aeaweb.org/conference/cont-ed/2020-webcasts)\n",
    "\n",
    "I'll also like to reference the amazing books from Angrist. They have shown me that Econometrics, or 'Metrics as they call it, is not only extremely useful but also profoundly fun.\n",
    "\n",
    "* [Mostly Harmless Econometrics](https://www.mostlyharmlesseconometrics.com/)\n",
    "* [Mastering 'Metrics](https://www.masteringmetrics.com/)\n",
    "\n",
    "My final reference is Miguel Hernan and Jamie Robins' book. It has been my trustworthy companion in the most thorny causal questions I had to answer.\n",
    "\n",
    "* [Causal Inference Book](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)\n",
    "\n",
    "The data that we used was taken from the article [Estimating Treatment Effects with Causal Forests: An Application](https://arxiv.org/pdf/1902.07409.pdf), by Susan Athey and Stefan Wager. \n",
    "\n",
    "## Contribute\n",
    "\n",
    "Causal Inference for the Brave and True is an open-source material on causal inference, the statistics of science. It uses only free software, based in Python. Its goal is to be accessible monetarily and intellectually.\n",
    "If you found this book valuable and you want to support it, please go to [Patreon](https://www.patreon.com/causal_inference_for_the_brave_and_true). If you are not ready to contribute financially, you can also help by fixing typos, suggesting edits or giving feedback on passages you didn't understand. Just go to the book's repository and [open an issue](https://github.com/matheusfacure/python-causality-handbook/issues). Finally, if you liked this content, please share it with others who might find it useful and give it a [star on GitHub](https://github.com/matheusfacure/python-causality-handbook/stargazers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
